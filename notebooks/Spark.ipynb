{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "390acbae-1c4a-49f3-9d34-b139cb9aeb58",
   "metadata": {},
   "source": [
    "# Query plans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989044e0-7fbd-4188-a237-2eefab336413",
   "metadata": {},
   "source": [
    "- how to read spark query plan "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4eecef91-0648-47b6-8498-50780b2f9568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e43fae3a-ebc9-4dd6-8207-7796fc26d085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.5.1'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a588da-ab0d-4d01-bd31-ff09171bfe15",
   "metadata": {},
   "source": [
    "# Spark context & spark session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7bb902-eca7-4c5e-9fef-eb7bb87238b7",
   "metadata": {},
   "source": [
    "- Spark session is meant for authentication\n",
    "- Spark context is meant for authorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a50e5-54cd-45cf-8a69-ae0fea5832ee",
   "metadata": {},
   "source": [
    "# Schema Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1cb9c1ae-207d-4ebd-9854-a4199b4b2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import * \n",
    "schema = StructType([StructField(\"author\", StringType(), False),\n",
    "                     StructField(\"title\", StringType(), False),\n",
    "                     StructField(\"pages\", IntegerType(), False)])\n",
    "\n",
    "# In Python DDL: schema = \"author STRING, title STRING, pages INT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3c741282-968c-461d-9740-36eb3e1bcef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "# Define schema for our data using DDL \n",
    "schema = \"`Id` INT, `First` STRING, `Last` STRING, `Url` STRING, `Published` STRING, `Hits` INT, `Campaigns` ARRAY<STRING>\"\n",
    "\n",
    "# Create our static data \n",
    "data = [[1, \"Jules\", \"Damji\", \"https://tinyurl.1\", \"1/4/2016\", 4535, [\"twitter\",\"51\", \"LinkedIn\"]], \n",
    "        [2, \"Brooke\",\"Wenig\", \"https://tinyurl.2\", \"5/5/2018\", 8908, [\"twitter\", \"LinkedIn\"]],\n",
    "        [3, \"Denny\", \"Lee\", \"https://tinyurl.3\", \"6/7/2019\", 7659, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [4, \"Tathagata\", \"Das\", \"https://tinyurl.4\", \"5/12/2018\", 10568, [\"twitter\", \"FB\"]],\n",
    "        [5, \"Matei\",\"Zaharia\", \"https://tinyurl.5\", \"5/14/2014\", 40578, [\"web\", \"twitter\", \"FB\", \"LinkedIn\"]],\n",
    "        [6, \"Reynold\", \"Xin\", \"https://tinyurl.6\", \"3/2/2015\", 25568, [\"twitter\", \"LinkedIn\"]]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "333d1e8a-cf62-45e9-8767-3b7cf67c5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7fa1597b-7c3a-4d11-b272-f82d07153d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName(\"Schemaexample2\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7aff9374-6566-46fd-8e14-6599dae5e6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf = SparkConf(loadDefaults=False)\n",
    "conf.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3c66d45a-355d-470f-9748-dd9771517933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame using the schema defined above\n",
    "blogs_df = spark.createDataFrame(data, schema) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a6129639-837b-4a4b-ae95-593e8fcb71fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "| Id|    First|   Last|              Url|Published| Hits|           Campaigns|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "|  1|    Jules|  Damji|https://tinyurl.1| 1/4/2016| 4535|[twitter, 51, Lin...|\n",
      "|  2|   Brooke|  Wenig|https://tinyurl.2| 5/5/2018| 8908| [twitter, LinkedIn]|\n",
      "|  3|    Denny|    Lee|https://tinyurl.3| 6/7/2019| 7659|[web, twitter, FB...|\n",
      "|  4|Tathagata|    Das|https://tinyurl.4|5/12/2018|10568|       [twitter, FB]|\n",
      "|  5|    Matei|Zaharia|https://tinyurl.5|5/14/2014|40578|[web, twitter, FB...|\n",
      "|  6|  Reynold|    Xin|https://tinyurl.6| 3/2/2015|25568| [twitter, LinkedIn]|\n",
      "+---+---------+-------+-----------------+---------+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame; it should reflect our table above \n",
    "blogs_df.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d54a5253-4896-493a-8f70-c7024741ad3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a76bd568-8370-4958-87d0-c323b66c7da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "LogicalRDD [Id#855, First#856, Last#857, Url#858, Published#859, Hits#860, Campaigns#861], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Id: int, First: string, Last: string, Url: string, Published: string, Hits: int, Campaigns: array<string>\n",
      "LogicalRDD [Id#855, First#856, Last#857, Url#858, Published#859, Hits#860, Campaigns#861], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "LogicalRDD [Id#855, First#856, Last#857, Url#858, Published#859, Hits#860, Campaigns#861], false\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[Id#855,First#856,Last#857,Url#858,Published#859,Hits#860,Campaigns#861]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aef4a2bc-b971-4ae3-b7b2-f886cd8bfaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: integer (nullable = true)\n",
      " |-- First: string (nullable = true)\n",
      " |-- Last: string (nullable = true)\n",
      " |-- Url: string (nullable = true)\n",
      " |-- Published: string (nullable = true)\n",
      " |-- Hits: integer (nullable = true)\n",
      " |-- Campaigns: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Print the schema used by Spark to process the DataFrame \n",
    "print(blogs_df.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a13d8564-801b-486c-9cf2-c96731117c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Scan ExistingRDD[Id#344,First#345,Last#346,Url#347,Published#348,Hits#349,Campaigns#350]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "blogs_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f9566-4360-4b51-8a78-660780ff53d4",
   "metadata": {},
   "source": [
    "# Spark Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4134b4-8f61-4790-8ec3-135f4111db00",
   "metadata": {},
   "source": [
    "## Dynamic allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96753a4-e228-4dd1-b365-ad1bd59b7540",
   "metadata": {},
   "source": [
    "- spark.dynamicAllocation.enabled true\n",
    "- spark.dynamicAllocation.minExecutors 2\n",
    "- spark.dynamicAllocation.schedulerBacklogTimeout 1m\n",
    "- spark.dynamicAllocation.maxExecutors 20\n",
    "- spark.dynamicAllocation.executorIdleTimeout 2min"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6587176-ef6f-4660-92a2-5ef8f3944f9c",
   "metadata": {},
   "source": [
    "By default spark.dynamicAllocation.enabled is set to false. When enabled with the settings shown here, the Spark driver will request that the cluster manager create two executors to start with, as a minimum (spark.dynamicAllocation.minExecu tors). As the task queue backlog increases, new executors will be requested each time the backlog timeout (spark.dynamicAllocation.schedulerBacklogTimeout) is exceeded. In this case, whenever there are pending tasks that have not been scheduled for over 1 minute, the driver will request that a new executor be launched to schedule backlogged tasks, up to a maximum of 20 (spark.dynamicAllocation.maxExecu tors). By contrast, if an executor finishes a task and is idle for 2 minutes (spark.dynamicAllocation.executorIdleTimeout), the Spark driver will terminate it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54e6545-cb10-4e58-9ba5-3370ee8a6f81",
   "metadata": {},
   "source": [
    "## Memory and the shuffle service"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132c1cc4-ddcf-46ff-a4cd-a37bcd04d652",
   "metadata": {},
   "source": [
    "![title](memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c074eb-008b-45e6-bef1-97cb69d8846c",
   "metadata": {},
   "source": [
    "# Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb7ef1c-b1f9-4324-83d9-ba3f735781b4",
   "metadata": {},
   "source": [
    "- To optimize resource utilization and maximize parallelism, the ideal is at least as many partitions as there are cores on the executor\n",
    "- a single thread running on a single core can work on a single partition\n",
    "- **How partitions**\n",
    "    - are created. As mentioned previously, Spark’s tasks process data as partitions read from disk into memory. Data on disk is laid out in chunks or contiguous file blocks, depending on the store. By default, file blocks on data stores range in size from 64 MB to 128 MB. For example, on HDFS and S3 the default size is 128 MB (this is configurable). A contiguous collection of these blocks constitutes a partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14788143-b290-47b0-b7e5-04db10fe0f8d",
   "metadata": {},
   "source": [
    "- The size of a partition in Spark is dictated by spark.sql.files.maxPartitionBytes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ca8e7c-318a-4e09-a65f-71bce7dd0897",
   "metadata": {},
   "source": [
    "### Shuffle partitions\n",
    "- shuffle partitions are created during the shuffle stage. By default, the number of shuffle partitions is set to **200** in **spark.sql.shuffle.partitions**. You can adjust this number depending on the size of the data set you have, to reduce the amount of small partitions being sent across the network to executors’ tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f64b68-5e6d-4fea-9548-7ace3cfa303d",
   "metadata": {},
   "source": [
    "# Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4c2f413-3a8d-469c-9acb-af8150dcc404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \n",
    "df = spark.range(1 * 10000000).toDF(\"id\")\n",
    "df.cache() # Cache the data \n",
    "df.count() # Materialize the cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8d63dcd1-12cd-4bf9-a5da-f771a4ad262e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() # Now get it from the cache res4: Long = 10000000 Command took 0.44 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30775e7c-2b4c-4ec2-a99d-eee5d3965acc",
   "metadata": {},
   "source": [
    "# Persist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975421d8-9957-4a83-988e-6f08461ad61c",
   "metadata": {},
   "source": [
    "- persist(StorageLevel.LEVEL) is nuanced, providing control over how your data is cached via StorageLevel. Table 7-2 summarizes the different storage levels. Data on disk is always serialized using either Java or Kryo serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e753313d-d562-4938-a90e-bd1ae0337e86",
   "metadata": {},
   "source": [
    "![title](persist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b3fcc3b-d19f-4a6f-a227-04d482357253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.storagelevel as sl\n",
    "# Create a DataFrame with 10M records  \n",
    "df = spark.range(1 * 100000000).toDF(\"id\")\n",
    "df.persist(sl.StorageLevel.DISK_ONLY) #Serialize the data and cache it on disk \n",
    "df.count() # Materialize the cache res2: Long = 10000000 Command took 2.08 seconds \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fa98d7b-190e-450c-9e3c-68b216516bc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000000"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count() #Now get it from the cache res3: Long = 10000000 Command took 0.38 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3113f2b-feba-43ec-a943-c8773427e87a",
   "metadata": {},
   "source": [
    "# Joins "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d906c856-e1ea-4591-80f9-c4f83ab4148c",
   "metadata": {},
   "source": [
    "## Broadcast join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757445a-7a81-4d22-b1e9-19cec815a1ce",
   "metadata": {},
   "source": [
    "- By default Spark will use a broadcast join if the smaller data set is **less than 10 MB**. This configuration is set **spark.sql.autoBroadcastJoinThreshold**\n",
    "- Specifying a value of **-1** in **spark.sql.autoBroadcastJoinThreshold** will cause Spark to always resort to a shuffle sort merge join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a343cfcf-cb88-4240-ba3e-1c17936ddc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/07/05 17:09:40 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/07/05 17:14:24 WARN TaskSetManager: Stage 3 contains a task of very large size (1534 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/07/05 17:14:32 WARN TaskSetManager: Stage 4 contains a task of very large size (2977 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 4:==============>                                          (4 + 12) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "|transaction_id|quantity|users_id|amount  |state|items|uid|login |email                |user_state|\n",
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "|4731          |4731    |0       |9462.0  |AZ   |SKU-2|0  |user_0|user_0@databricks.com|TX        |\n",
      "|11856         |11856   |0       |23712.0 |CA   |SKU-1|0  |user_0|user_0@databricks.com|TX        |\n",
      "|12050         |12050   |0       |24100.0 |CA   |SKU-3|0  |user_0|user_0@databricks.com|TX        |\n",
      "|17285         |17285   |0       |34570.0 |CO   |SKU-3|0  |user_0|user_0@databricks.com|TX        |\n",
      "|24361         |24361   |0       |48722.0 |CO   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|24867         |24867   |0       |49734.0 |TX   |SKU-2|0  |user_0|user_0@databricks.com|TX        |\n",
      "|32284         |32284   |0       |64568.0 |CA   |SKU-4|0  |user_0|user_0@databricks.com|TX        |\n",
      "|40669         |40669   |0       |81338.0 |AZ   |SKU-3|0  |user_0|user_0@databricks.com|TX        |\n",
      "|42100         |42100   |0       |84200.0 |CA   |SKU-4|0  |user_0|user_0@databricks.com|TX        |\n",
      "|55507         |55507   |0       |111014.0|CO   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|78065         |78065   |0       |156130.0|TX   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|98780         |98780   |0       |197560.0|NY   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|100680        |100680  |0       |201360.0|CO   |SKU-2|0  |user_0|user_0@databricks.com|TX        |\n",
      "|102427        |102427  |0       |204854.0|CA   |SKU-1|0  |user_0|user_0@databricks.com|TX        |\n",
      "|105915        |105915  |0       |211830.0|CO   |SKU-2|0  |user_0|user_0@databricks.com|TX        |\n",
      "|109758        |109758  |0       |219516.0|CA   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|111015        |111015  |0       |222030.0|TX   |SKU-4|0  |user_0|user_0@databricks.com|TX        |\n",
      "|113551        |113551  |0       |227102.0|CO   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|123478        |123478  |0       |246956.0|AZ   |SKU-0|0  |user_0|user_0@databricks.com|TX        |\n",
      "|132936        |132936  |0       |265872.0|AZ   |SKU-4|0  |user_0|user_0@databricks.com|TX        |\n",
      "+--------------+--------+--------+--------+-----+-----+---+------+---------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import random\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Join Example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set autoBroadcastJoinThreshold to -1 to disable broadcast join\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "# Define sample data for states and items\n",
    "states = {0: \"AZ\", 1: \"CO\", 2: \"CA\", 3: \"TX\", 4: \"NY\", 5: \"MI\"}\n",
    "items = {0: \"SKU-0\", 1: \"SKU-1\", 2: \"SKU-2\", 3: \"SKU-3\", 4: \"SKU-4\", 5: \"SKU-5\"}\n",
    "\n",
    "# Generate sample data for usersDF\n",
    "users_data = [(id, f\"user_{id}\", f\"user_{id}@databricks.com\", states[random.randint(0, 4)]) for id in range(1000001)]\n",
    "usersDF = spark.createDataFrame(users_data, [\"uid\", \"login\", \"email\", \"user_state\"])\n",
    "\n",
    "# Generate sample data for ordersDF\n",
    "orders_data = [(r, r, random.randint(0, 9999), 10 * r * 0.2, states[random.randint(0, 4)], items[random.randint(0, 4)]) for r in range(1000001)]\n",
    "ordersDF = spark.createDataFrame(orders_data, [\"transaction_id\", \"quantity\", \"users_id\", \"amount\", \"state\", \"items\"])\n",
    "\n",
    "# Perform the join operation\n",
    "usersOrdersDF = ordersDF.join(usersDF, col(\"users_id\") == col(\"uid\"))\n",
    "\n",
    "# Show the joined results\n",
    "usersOrdersDF.show(truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e6ba5110-7633-4f27-a705-c2400721aa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- SortMergeJoin [users_id#701L], [uid#691L], Inner\n",
      "   :- Sort [users_id#701L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(users_id#701L, 200), ENSURE_REQUIREMENTS, [plan_id=1009]\n",
      "   :     +- Filter isnotnull(users_id#701L)\n",
      "   :        +- Scan ExistingRDD[transaction_id#699L,quantity#700L,users_id#701L,amount#702,state#703,items#704]\n",
      "   +- Sort [uid#691L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(uid#691L, 200), ENSURE_REQUIREMENTS, [plan_id=1010]\n",
      "         +- Filter isnotnull(uid#691L)\n",
      "            +- Scan ExistingRDD[uid#691L,login#692,email#693,user_state#694]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersOrdersDF.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8635b9-fdd7-4bb4-95ab-50a02e23f48a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
